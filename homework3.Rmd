---
title: "R Notebook"
output: html_notebook
---

We have a sampling of $n$ independent observation from a gaussian *joint* distribution of dimension $p$. These data are stored into a matrix $\mathbb{X} \in \mathbb{R}^{n\times p}$.<br>

Given the matrix we want to discover the unknown adiacency matrix that models the DAG of the joint distribution that has generated our observations.

This matrix is $\mathbb{A}$ and it's a lower triangular matrix that belongs to $\mathbb{R}^{p\times p}$ and it's such that it has a 0 in the position $(i,j)$ if $j \not\in pa(X_i)$ else the value means the strenght of the relation between these two nodes.<br>

We know how to model our observation, we know what $\mathbb{A}$ means, but how can we link the distribution of the j-th random variable (remember that we have a p-dimensional random vector) to the matrix $\mathbb{A}$?<br>
Well we can say that the j-th random variable $X_j$ can be computed as follows:
\begin{equation}
  X_j = \sum_{k | k \neq j}\mathbb{A[j,k]\cdot X_k}+\epsilon_j
\end{equation}
The gaussian is in $\epsilon$, in fact $\epsilon_j \sim N_1(0, \sigma^2)$.

So we have a model that depends from two parameters that are $\mathbb{A}$ and $\sigma^2$. At this point we can proceed to the things that are of interest for us.

Since we can obtain the distributions of the X from our parameters, we want to discover our parameter starting from our observations, so the problem reduces to find $\theta = (\mathbb{A}, \sigma^2)$ starting from $\mathbb{X}$.

In order to do this we can use the log likelihood that's defined as follows:
\begin{equation}
  \mathcal{\ell}_n(\mathbb{A}, \sigma^2) \propto - \sum_{j=1}^p \left (\frac{1}{2\sigma^2} \sum_{i=1}^n \left( \mathbb{X}[i,j] - \sum_{k | k\neq j} \mathbb{A}[j,k]\cdot \mathbb{X}[i,k]\right)^2 + \frac{n}{2}ln\sigma^2 \right)
\end{equation}
Let's enroll it:
We have three sums that are, going from the inner to the outer:

* *inner*: $\sum_{k | k\neq j} \mathbb{A}[j,k]\cdot \mathbb{X}[i,k]$ where we are doing the dot product from the weight that has the eventual edge that goes from $k$ to $j$ (scalar) and the value of the k-th random variable ($X_k$) in the i-th sample, than we sum all these values. This can be seen as a kind of a weighted sum of the i-th sample where the weights are the influences that has the variable j to the other random variables. So it can be seen as a "how much j has influenced in total the sample $i$"
* *middle*: $\sum_{i=1}^n \left( \mathbb{X}[i,j] - \text{inner}\right )^2$ where for each of the i-th observation of the j-th random variable, we subtract the sum obtained in the inner that is a scalar, than we sum all these values. We are subtracting the value of the j-th random variable in the sample i with the influence that has had this variable over the other variable, than we do the square of this difference, so it's hight either if $X_j$ have a big value in the i-th sample without influencing too much the other variable, or if the variable has a low value (maybe negative) and has a big influence on the other ones
* *outer*: $\sum_{j=1}^p \frac{\text{middle}}{2\sigma^2} + \frac{n}{2}ln\sigma^2$ We sums up the value obtained from all the $p$ random variables multiplicated and summed with constants that are dependent from $\sigma^2$

Let's see it in code:


```{r}
log_likelihood = function(X, A, sigma.square){
  #cat('log_likelihood from A: ', dim(A),' sigma: ', sigma.square)
  tot = 0
  n = dim(X)[1]
  p = dim(X)[2]
  for(j in 1:p){
    
    # Finding the indexes K = {k s.t. k is in [1,p]\j}
    K = which((1:p)!=j)
    # in k we have the indexes of the row without j
      
    inners = apply(X[, K] * A[j, K], 1, sum) # n length vector containing the inner sums for each i-th sample
    middle = sum((X[, j] - inners)^2)  # as a matter of fact in the middle we are working on the j-th columns
    tot = tot - (middle/(2*sigma.square) + (n/2*log(sigma.square)))
  }
  return(tot)
}
```

Since MLEdag computes for us the estimate of the matrix $\mathbb{A}$ and the matrix $\mathbb{X}$ is what we have given that it's our observation, we can optimize the log likelihood in order to retrieve sigma, so it'll be the sigma.MLE

```{r}
sigma.MLE = function(X, A)
  return(
    optim(
      par=c(0.01),
      fn=function(sigma.sq)
        log_likelihood(X,A, sigma.sq),
      method=c("Brent"),
      lower = 0,
      upper = 10000,
            control = list(fnscale = -1)
    )
  )
```

According with [Chunlin Li Et. al](https://elearning.uniroma1.it/pluginfile.php/1112893/mod_assign/introattachment/0/Likelihood%20Ratio%20Tests%20for%20a%20Large%20Directed%20Acyclic%20Graph.pdf?forcedownload=1) we can also aproximate $\sigma^2$ with:
\begin{equation}
  \hat{\sigma}^2 = (np)^{-1} \sum_{j=1}^p \sum_{i=1}^n\left ( \mathbb{X}[i,j] - \sum_{k | k\neq j} \mathbb{A}[j,k]\cdot \mathbb{X}[i,k] \right ) ^2
\end{equation}

In code we have:

```{r}
sigma.estimator = function(X, A){
  n = dim(X)[1]
  p = dim(X)[2]
  sigma = 0
  for(j in 1:p){
    K = which((1:p)!=j)
    inners = apply(X[, K] * A[j, K], 1, sum)
    sigma = sigma + sum((X[,j]-inners)^2)
  }
  return(sigma/(n*p))
}
```

Back to our business... We have a way to estimate $\mathbb{A}$ by the clrdag library's function MLEdag and, once we have it we can now compute $\sigma^2$ in the two way of above.
Let's now generate our observation $\mathbb{X}$ using an adiacency matrix $\mathbb{A}$ that for the moment we suppose to be **unknown**:
```{r}
# Suppose to have 100 observation of 10 random variables
n = 100
p = 10
sparsity <- 1/p

generate.X = function(p, n, A=NULL, D=NULL, h0=T, linkages=T){
  # If A is not provided I'll generate it accordingly to eventual links and hypotesis sake
  if(is.null(A)){
    A = matrix(rbinom(p*p,1,sparsity)*sign(runif(p*p,min=-1,max=1)),p,p)
    A[upper.tri(A, diag = T)] = 0
    if(!is.null(D))
      if(h0)
        A[D==1] = 1-as.integer(linkages) #A[j,k] = 0 for ALL linkages in F if we want to test the linkages, else 1
      else{
        A[sample(which(D==1), sample(sum(D==1), 1))] = as.integer(linkages) #A[j,k] = 0 for ALL the links in F if we are NOT testing the linkages
        cat('for invalidate h0 A has in positions', which(D==1), ' ', sum(A[which=D==1]), ' ones')
      }
  }

  return(matrix( rnorm(n*p), n, p) %*% t(solve(diag(p) - A)))
}

# Generating the hidden Adaicency matrix
A.hidden = matrix(rbinom(p*p,1,sparsity)*sign(runif(p*p,min=-1,max=1)),p,p)
A.hidden[upper.tri(A.hidden, diag = T)] = 0
A.hidden[2, 1] = 0
A.hidden[6, 3] = 0
A.hidden[10, 9] = 0

# X is generated gaussian mantaining the dependencies described by the adiacency matrix A
X = matrix( rnorm(n*p), n, p) %*% t(solve(diag(p) - A.hidden) ) 
#X =generate.X(p, n, D=D, h0=T, linkages = T)
U_n.linkages(X, D)

```



We've said that A is unknown, so if we want to test the existence of some link in the Dag or some specific pathway we can't look into it, so we need a test.

## Test the linkages in X
If we want to see if a set of linkages $\mathcal{F} = \{(j,k)\}$ belongs to the Graph we can set up a test over $\mathbb{A}$ where:
\begin{equation}
  H_0 : \mathbb{A}[j,k] = 0 \forall (j,k) \in \mathcal{F}
\end{equation}
Once we have this, we can split \mathbb{X} in train and set, in the train half of observation we'll compute the **constrained** MLEs for $\mathbb{A}$ and $\sigma^2$ accordingly to $H_0$, namely $\hat{\theta}_0^{tr}$, instead with the testing half of our observation we'll compute the **unconstrained** MLEs, namely $\hat{\theta}^{te}$.

In order to do this we must compute:
* $\hat{\theta}_0^{tr} = (\hat{\mathbb{A}}_0, \hat{\sigma}_0)$
* $\hat{\theta}^{te} = (\hat{\mathbb{A}}, \hat{\sigma})$

Given these ingredients we can substitute these thetas into the likelihoods and obtain the **split Likelihood ratio**:
\begin{equation}
  U_n = \frac{\mathcal{L}(\hat{\theta}^{te} | \mathbb{X}^{tr})}{\mathcal{L}(\hat{\theta}_0^{tr} | \mathbb{X}^{tr})}
\end{equation}
**IF** $U_n \gt \frac{1}{\alpha}$ then we can **reject** $H_0$.<br>

Since we have the log likelihood, we can easily transform our disequality into a more manageable form:
\begin{align}
  &U_n > \frac{1}{\alpha}\\
  &\iff \frac{\mathcal{L}(\hat{\theta}^{te} | \mathbb{X}^{tr})}{\mathcal{L}(\hat{\theta}_0^{tr} | \mathbb{X}^{tr})} \gt \frac{1}{\alpha}\\
  &\iff \mathcal{\ell}(\hat{\theta}^{te} | \mathbb{X}^{tr}) - \mathcal{\ell}(\hat{\theta}_0^{tr} | \mathbb{X}^{tr}) \gt - ln(\alpha)
\end{align}

Suppose that we have the following set of linkages of which we want to test the existence: $\mathcal{F}=\{  (2, 1), (6, 3), (10, 9)\}$
So our $H_0$ is **any** $\mathbb{A}$ such that **all** the tuples in $\mathcal{F}$ have a zero in $\mathbb{A}$.<br>
This can be computed using MLEdag and passing a mask matrix of size $(p \times p)$ that has all zeros and has ones only in the position of the linkages for which we want to build the test, the unconstrained $\mathbb{A}$ matrix is the one obtained by MLEdag without passing any constraint over the edges (matrix D). So we'll compute the two version of the $\mathbb{A}$ matrix and we'll compute the relatives $\sigma^2$. In the following cell we show also that the two ways to compute $\sigma^2$ starting from $\mathbb{A}$ and $\mathbb{X}$ are equivalent.
```{r}
# Building the mask matrix D
library(clrdag)
D = matrix(0, p, p)
D[2, 1] = 1
D[6, 3] = 1
D[10, 9] = 1

#X = generate.X(p, n, D = D, h0=T, linkages = T)
#Splitting the observations into test and train
X.tr = X[1:(n%/%2),]
X.te = X[(n%/%2+1):n,]

# Obtaining the A MLEs
A.h0 = MLEdag(X = X.tr, D = D, tau = 0.35, mu = 1, 
                    rho = 1.2, trace_obj = FALSE)$A.H0
A = MLEdag(X = X, tau = 0.35, mu = 1, 
                    rho = 1.2, trace_obj = FALSE)$A


# Obtaining the sigma MLEs
sigma.h0 = sigma.estimator(X.tr, A.h0)
sigma.h0.mle = sigma.MLE(X.tr, A.h0)

sigma.unconstrained.mle = sigma.MLE(X.te, A)
sigma.unconstrained = sigma.estimator(X.te, A)

comparison = matrix(c(sigma.h0, sigma.h0.mle$par, sigma.unconstrained, sigma.unconstrained.mle$par), nrow=2, byrow=T)
colnames(comparison) = c('Proxy', 'MLE estimate')
```


```{r}
data.frame(comparison)
``` 
Now that we have $\hat{\theta}_0^{tr}$ and $\hat{\theta}^{te}$ we can compute $U_n$. Since it is the ratio of two likelihoods, its logarithm can be computed as the difference between two log-likelihood, so we'll do the exponential of this difference.
```{r}()
U_n = exp(log_likelihood(X.tr, A, sigma.unconstrained)-log_likelihood(X.tr, A.h0, sigma.h0))
U_n
```
### Putting all togheter
Since the value of $U_n$ depends on the sample X and on the linkages that we want to test, we can wrap all these steps into a unique function that gives us the value of $U_n$ and this is useful to make some statistic on it (point 3 of the homework).

```{r}
U_n.linkages = function(X, D){
  n = dim(X)[1]
  X.tr = X[1:(n%/%2),]
  X.te = X[(n%/%2+1):n,]
  
  tmp = MLEdag(X = X.tr, D = D, tau = 0.35, mu = 1, 
                    rho = 1.2, trace_obj = FALSE)
  A.h0 = tmp$A.H0
  A = tmp$A.H1
  #A = MLEdag(X = X.te, tau = 0.35, mu = 1,rho = 1.2, trace_obj = FALSE)$A
  
  sigma.h0 = sigma.estimator(X.tr, A.h0)
  sigma.unconstrained = sigma.estimator(X.te, A)
  
  return(log_likelihood(X.tr, A, sigma.unconstrained)-log_likelihood(X.tr, A.h0, sigma.h0))
}
```


```{r}
U_n.path= function(X, D){
  n = dim(X)[1]
  X.tr = X[1:(n%/%2),]
  X.te = X[(n%/%2+1):n,]
  
  # Computing all the A.h0 over the sparsity parameters 'k'
  A.mles = lapply(1:sum(D), function(mu)MLEdag(X.tr, D=D, tau=0.35, mu=mu, rho=1.2, trace_obj = F))
  A.h0s  = lapply(A.mles, function(a)a$A.H0)
  A.h1s  = lapply(A.mles, function(a)a$A.H1)
  # For each A.h0 compute the associated sigma
  sigmas.0 = t(sapply(A.h0s, function(a)sigma.estimator(X.tr, a)))
  sigmas.1 = t(sapply(A.h1s, function(a)sigma.estimator(X.te, a)))
  # Computing the likelihoods of each pair (A.h0, sigma.h0)
  likelihoods.0 = t(sapply(1:length(sigmas.0), function(idx)log_likelihood(X.tr, A.h0s[[idx]], sigmas.0[idx])))
  likelihoods.1 = t(sapply(1:length(sigmas.1), function(idx)log_likelihood(X.tr, A.h1s[[idx]], sigmas.1[idx])))
  
  # Finding the idx that give us the maximum likelihood
  mle_idx.0 = which(likelihoods.0==max(likelihoods.0))[1] # maximum likelihood over h0
  mle_idx.1 = which(likelihoods.1==max(likelihoods.1))[1] # maximum likelihood over h0
  
  A= MLEdag(X = X.te, tau = 0.35, mu = 1, 
                  rho = 1.2, trace_obj = FALSE)$A
  sigma.unconstrained = sigma.estimator(X.te, A)
  return (likelihoods.1[mle_idx.1]-likelihoods.0[mle_idx.0])
  #return(list(U.n=exp(log_likelihood(X.tr, A, sigma.unconstrained)-likelihoods[mle_idx]), A.h0=A.h0s[[mle_idx]]))
}
```


```{r}
library(clrdag)
D.pathway = matrix(0, p,p)
D.pathway[3, 4]=1
D.pathway[4, 5]=1
D.pathway[5, 6]=1
D.pathway[6, 7]=1

A.pathway.hidden = matrix(rbinom(p*p,1,sparsity)*sign(runif(p*p,min=-1,max=1)),p,p)
A.pathway.hidden[upper.tri(A.hidden, diag = T)] = 0
A.pathway.hidden[4,3]=1
A.pathway.hidden[5,4]=1
A.pathway.hidden[6,5]=1
A.pathway.hidden[7,6]=1
X.pathway = matrix( rnorm(n*p), n, p) %*% t(solve(diag(p) - A.pathway.hidden))
#X.pathway = generate.X(p, n, D=D, h0=F, linkages = F)
out= U_n.path(X.pathway, D.pathway)
#abs(out$A.h0-A.hidden)
out#$U.n #log 0.05= -2.995732
```

```{r}
inner_LRT_function.paths = function(X.tr, X.te, D){
   
  # Computing all the A.h0 over the sparsity parameters 'k'
  A.mles = lapply(1:sum(D), function(mu)MLEdag(X.tr, D=D, tau=0.35, mu=mu, rho=1.2, trace_obj = F))
  A.h0s  = lapply(A.mles, function(a)a$A.H0)
  A.h1s  = lapply(A.mles, function(a)a$A.H1)
  # For each A.h0 compute the associated sigma
  sigmas.0 = t(sapply(A.h0s, function(a)sigma.estimator(X.tr, a)))
  sigmas.1 = t(sapply(A.h1s, function(a)sigma.estimator(X.te, a)))
  # Computing the likelihoods of each pair (A.h0, sigma.h0)
  likelihoods.0 = t(sapply(1:length(sigmas.0), function(idx)log_likelihood(X.tr, A.h0s[[idx]], sigmas.0[idx])))
  likelihoods.1 = t(sapply(1:length(sigmas.1), function(idx)log_likelihood(X.tr, A.h1s[[idx]], sigmas.1[idx])))
  
  # Finding the idx that give us the maximum likelihood
  mle_idx.0 = which(likelihoods.0==max(likelihoods.0))[1] # maximum likelihood over h0
  mle_idx.1 = which(likelihoods.1==max(likelihoods.1))[1] # maximum likelihood over h0
  
  A= MLEdag(X = X.te, tau = 0.35, mu = 1, 
                  rho = 1.2, trace_obj = FALSE)$A
  sigma.unconstrained = sigma.estimator(X.te, A)
  return (likelihoods.1[mle_idx.1]-likelihoods.0[mle_idx.0])
  #return(list(U.n=exp(log_likelihood(X.tr, A, sigma.unconstrained)-likelihoods[mle_idx]), A.h0=A.h0s[[mle_idx]]))
}

inner_LRT_function.links = function(X.tr, X.te, D){
  
  tmp = MLEdag(X = X.tr, D = D, tau = 0.35, mu = 1, 
                    rho = 1.2, trace_obj = FALSE)
  A.h0 = tmp$A.H0
  A = tmp$A.H1
  
  sigma.h0 = sigma.estimator(X.tr, A.h0)
  sigma.unconstrained = sigma.estimator(X.te, A)
  
  return(log_likelihood(X.tr, A, sigma.unconstrained)-log_likelihood(X.tr, A.h0, sigma.h0))
}

log.LRT = function(X,D, links=T){
  n=dim(X)[1]
  X.tr = X[1:(n%/%2),]
  X.te = X[(n%/%2+1):n,]
  
  if(links){
  U_n = inner_LRT_function.links(X.tr, X.te, D)
  U_n.swap = inner_LRT_function.links(X.te, X.tr, D)
  }
  else{
    U_n = inner_LRT_function.paths(X.tr, X.te, D)
    U_n.swap = inner_LRT_function.paths(X.te, X.tr, D)
  }
  return(list(links = links, U_n = U_n, W_n = (log((exp(U_n)+exp(U_n.swap))/2))))
}

```




```{r}
library(cli)
library(clrdag)
test_path = function(m=1e2,h0=T, alpha = 0.05){
  p = 10
  n = 100
  sparsity = 2/p
  D.pathway = matrix(0, p,p)
  D.pathway[3, 4]=1
  D.pathway[4, 5]=1
  D.pathway[5, 6]=1
  D.pathway[6, 7]=1
  
  A.pathway.hidden = matrix(rbinom(p*p,1,sparsity)*sign(runif(p*p,min=-1,max=1)),p,p)
  A.pathway.hidden[upper.tri(A.pathway.hidden, diag = T)] = 0
  A.pathway.hidden[4,3]=as.integer(!h0)
  A.pathway.hidden[5,4]=1
  A.pathway.hidden[6,5]=1
  A.pathway.hidden[7,6]=1
  out_U = 0
  out_W = 0
  cli_progress_bar("Cleaning data", total = m)
  for(i in 1:m){
    X = matrix( rnorm(n*p), n, p) %*% t(solve(diag(p) - A.pathway.hidden) )
    LRT = log.LRT(X, D.pathway, links=F)
    if(LRT$links)
      cat('ERRORE, chiamata LRT con links=', links,'\n')
    out_U = out_U+(LRT$U_n>-log(alpha))
    out_W = out_W+(LRT$W_n>-log(alpha))
    cli_progress_update()
  }
  cli_progress_done()
  return(list(U_n = out_U/m, W_n = out_W/m))
}

test_link = function(m=1e3, alpha=.05, h0=T){
  p = 10
  n= 100
  sparsity = 2/p
  
  D.link = matrix(0, p, p)
  D.link[2, 1] = 1
  D.link[6, 3] = 1
  D.link[10, 9] = 1
  
  A.link.hidden = matrix(rbinom(p*p,1,sparsity)*sign(runif(p*p,min=-1,max=1)),p,p)
  A.link.hidden[upper.tri(A.link.hidden, diag = T)] = 0
  A.link.hidden[2, 1] = as.integer(!h0)
  A.link.hidden[6, 3] = 0
  A.link.hidden[10, 9] = 0
  
  out_U = 0
  out_W = 0
  cli_progress_bar("Cleaning data", total = m)
  for(i in 1:m){
    X = matrix( rnorm(n*p), n, p) %*% t(solve(diag(p) - A.link.hidden) )
    LRT = log.LRT(X, D.link, links=T)
    if(!LRT$links)
      cat('ERRORE, chiamata LRT con links=', links,'\n')
    out_U = out_U+(LRT$U_n>-log(alpha))
    out_W = out_W+(LRT$W_n>-log(alpha))
    cli_progress_update()
  }
  cli_progress_done()
  return(list(U_n=out_U/m, W_n=out_W/m))
}
```


```{r}
set.seed(1234)
test_path()
```

```{r}
set.seed(1234)
test_link()
```


